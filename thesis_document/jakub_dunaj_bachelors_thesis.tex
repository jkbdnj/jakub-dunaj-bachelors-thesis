%%%%%% LaTeX template file for the Bachelor's thesis
%%%%%% Faculty of Civil and Environmental Engineering, TU Wien
%%%%%% The file BachelorBUI.cls must be in the same folder
%%
%% created by Christian Schranz and Sebastian Pech
%% CEE Computer Lab & Digital Building Process
%% date: 2022-10-01
%% tested for pdflatex
%%
\documentclass{BachelorBUI}
%%
%% Packages already loaded in the above document class
%%
%% fontenc[T1], lmodern, microtype, babel[english,ngerman], graphicx,
%% geometry with all margins or areaset (choose what you like)
%% mathtools, amssymb, xfrac, siunitx, booktabs,
%% url, xcolor[table], textcomp, marvosym, pifonts, pdfpages, ragged2e,
%% tabularx, longtable, threeparttable, csquotes, eurosym, enumitem,
%% multirow, setspace, listings, scrlayer-scrpage with header/footline
%% pdfx (including hyperref)
%%
\usepackage[utf8]{inputenc}
\RequirePackage[babel,english=american]{csquotes} %% context sensitive quotations
\raggedbottom
\lstset{
 language={Matlab},
}
\sisetup{output-decimal-marker = {,},
range-phrase = --,
group-separator = {~},
per-mode = symbol,
list-final-separator={ and }}
\graphicspath{{images/}}
\newcommand{\eg}{\mbox{e.\,g.}\xspace}
\newcommand{\Name}[1]{\textsc{#1}}
\newcommand{\vKTxv}{\mathbf{v}_1^T\tilde{\mathbf{K}}_{T},_{\xi}\mathbf{v}_1}
\newcommand{\vKTxxv}{\mathbf{v}_1^T\tilde{\mathbf{K}}_{T},_{\xi\xi}\mathbf{v}_1}
%% biblatex and biber for the bibliography
%% set different citation variants via options:
%% style=numeric-comp ... [1]
%% style=authoryear ... Mang 1998 / use \textcite{} ... Mang (1998)
%%
\usepackage[style=authoryear,backend=biber,maxcitenames=2]{biblatex}
\ExecuteBibliographyOptions{%
 giveninits=true,maxbibnames=99}%
\DefineBibliographyStrings{english}{%
andothers={et\;al\adddot},
urlseen = {Accessed on}
}
\addbibresource{references.bib}
\usepackage{acro}
\acsetup{list/display=used}
\input{acronyms}
% URL hyphenation rules:
\setcounter{biburllcpenalty}{9000}% Lowercase
\setcounter{biburlucpenalty}{9000}% Uppercase
% (see https://texwelt.de/fragen/7008/zeilenumbruche-in-bibliografielinks )
%% Now enter a title
\title{Web-based Application for Plant Disease Classification using Convolutional Neural Networks}
\authorname{Jakub Dunaj} %% Enter your name here
\email{e12121285@student.tuwien.ac.at} %% Enter your email address here
\MatrNr{12121285} %% Enter your student ID number here
\thesislanguage{en-US} %% Language of the thesis (de-AT, de-DE, en-GB, en-US)
\keywords{bachelor thesis\sep template\sep LaTeX}
%% Package pdfx currently incorrectly implemented ... therefore pdfx commented out and hyperref loaded instead
%\usepackage[a-2u,pdf15]{pdfx}
% here the document begins
\begin{document}
\selectlanguage{english}
%% ========== Key Words ==========   
%%
%% Enter some keywords with the \Keywords command and
%% separate them with the \sep command%%
\begin{filecontents}[overwrite]{\jobname.xmpdata}
\makeatletter
\Title{\@title}
\Author{\@authorname}
\Language{\@thesislanguage}
\Keywords{\@keywords}
\Publisher{TU Wien}
\makeatother
\end{filecontents}
%%%%%%%%%
%% Title
%%%%%%%%%
%
\maketitle

% Abstract
\begin{abstract}

Here comes the abstract.

\end{abstract}
% Table of contents
\tableofcontents
\section{Introduction}
\begin{itemize}
    \item different types of plant diseases and their impact on yield loss
\end{itemize}
\section{Literature Review}
To conduct a comprehensive and quality-focused search for scientific literature on the topic of my bachelor's thesis, I utilized two reputable scientific databases: Scopus and IEEE Explore. 
I employed a key-based approach to filter out the most relevant literature, using the following carefully created search queries: 
\begin{itemize}
    \item \textit{( "crop disease" OR "plant disease" ) AND ( "detection" OR "classification" OR "identification") AND ( "deep learning" )}, 
    \item \textit{( "crop disease" OR "plant disease" ) AND ( "detection" OR "classification" OR "identification") AND ( "deep learning" )  AND ("survey" OR "review")}, 
    \item \textit{( "CNN" ) AND ( "deep learning" ) AND ( "crop disease" OR "plant disease" )},
    \item \textit{( "CNN" ) AND ( "deep learning" ) AND ( "crop disease" OR "plant disease" ) AND ( "review" OR "survey" )},
    \item \textit{( "CNN" ) AND ( "deep learning" ) AND ( "review" OR "survey" )}.
\end{itemize}
% TODO: add citations to the papers (e.g. for the mentioned architectures)
When creating these search queries, I deliberately avoided using too restrictive or too generic keywords or phrases. I have designed the search queries to be specific enough to filter out the irrelevant literature while remaining general enough to keep potentially relevant results. After this initial filtering, I further refined the results based on the publication date and the number of citations. Despite using these methods, many search results failed to meet the criteria for a high-quality scientific papers. This became frequently apparent in the faulty usage of English and poor quality of text, pictures, tables, and figures. To overcome this problem, my additional criteria for result selection included the quality of the journal or conference and the reputation of the authors' affiliated institutions. I used this selection process not only for the overall search results returned by the database, but also to a subset of the search results that were filtered by the publication year. My aim was not only to capture the most current knowledge in the researched field, but also to document its successive advances. In addition, I employed the "snowballing" method, using references from already identified papers to discover additional relevant literature. This consistent and rigorous methodology yielded the most relevant, current, and influential papers on crop and plant disease detection using deep learning methods. In summary, my selection process considered not only the citation counts but also the reputations of authors and their affiliated institutions. Consequently, I sometimes selected papers with lower citation counts if they were produced by highly reputable authors or institutions. In the following paragraphs, I will present and review the selected scientific papers to provide a comprehensive and quality-focused overview of the evolution, current advances, and future perspectives in crop and plant disease detection using convolutional neural networks as a deep learning method.

\textcite{Sladojevic:2016} introduced a new approach to detecting plant diseases from leaf images, leveraging the advances in convolutional neural networks and their image classification capabilities. \textcite{Sladojevic:2016} developed a model capable of classifying 13 different plant-disease pairs, one class of healthy leaves and one of leaves without background. The authors created an initial dataset of 13 classes representing different plant-disease combinations by collecting leaf images from the internet. They then added two more classes to the dataset: one containing only images of healthy plant leaves, and another containing images of plant leaves without background. This initial dataset was then preprocessed by cropping the plant leaf images and resizing the images to reduce the training time. This preprocessed dataset was then augmented in order to create large enough dataset. The authors of the paper applied different transformations techniques including affine transformations, perspective transformations, and image rotations. This augmentation phase introduced more variance into the dataset to reduce the overfitting during the training phase. Using the described methods, the authors were able to create a database consisting of 30,880 plant leaf images in the training subset and of 2,589 images in the testing subset. The authors utilized the transfer learning method. To modify the CaffeNet architecture, which was pre-trained on the ImageNet dataset, for the classification of the 15 classes in the created dataset, the authors modified the network's final layer. The trained model achieved the precision between 91\% and 98\% for the different classes. The final overall accuracy was reported to be 96.3\%.

\textcite{Mohanty:2016} utilized the public PlantVillage dataset, initially presented in (\cite{Hughes:2015}), to train two different convolutional neural network architectures, GoogLeNet and AlexNet. The authors trained both architectures under various settings and compared the performances of the trained models based on their ability to detect different plant-disease pairs or healthy leaves. The dataset consisted of 54,306 leaf images of healthy and diseased plants. In total, there were 38 classes created from 14 plants and 26 different diseases in the dataset. The classes corresponded to a plant-disease pair or were dedicated solely to healthy leaves of particular plant species. The healthy or diseased leaf images in the dataset were taken under controlled conditions with neutral background. Across all the experiments the authors conducted, three different versions of the whole dataset were used to train the convolutional neural networks. The authors used the dataset in its original colored version, in its gray-scaled version, and in its segmented version. The segmented version of the original dataset was created by removing all the extra background information in the leaf images that may introduce some bias in the dataset was removed. During the segmentation process the color shift in the leaf images were fixed removing another potential bias in the dataset. Each of these dataset variations was then split into five different dataset configurations used for training and testing of the architectures: 80\% of the dataset used for training and 20\% used for testing, 60\% of the dataset used for training and 40\% used for testing, 50\% of the dataset used for training and 50\% used for testing, 40\% of the dataset used for training and 60\% used for testing, 20\% of the dataset used for training and 80\% used for testing. Furthermore, the authors trained both of the architectures on these datasets from scratch in one case and by adapting these architectures that were already pre-trained on the ImageNet dataset using transfer learning in the other case. In total, these various conditions - two different CNN architectures, three different versions of the PlantVillage dataset, five different splits of each of these dataset versions into train and test subsets, and two different train approaches - resulted in 60 different experiments. The hyper-parameter configurations of the training process were kept the same for all of the conducted experiments. The overall accuracy across all of the experiment configurations ranged from 85.53\% to 99.34\%. In the first case of the 85.53\% overall accuracy the AlexNet architecture was trained from scratch on the gray-scale version of the dataset and 80\% of the dataset images were used for training and the remaining 20\% of the dataset were used for the testing of the trained model. In the other case of 99.34\% overall accuracy the GoogLeNet architecture was trained on the original colored version dataset using transfer learning. 80\% of this original dataset were used for training of the model and the remaining 20\% were used for testing. The authors of the paper noticed that with the increase in the train set to test set ration the trained models consistently perform better. The authors also noticed that the models trained on the original colored dataset outperformed the models that were trained using the same configuration but different dataset variation. The exact same behavior was observed by the authors when comparing the models that were trained using the transfer learning to the models trained from scratch. The models trained using transfer learning outperformed those trained from scratch when all other parameters were kept constant. The authors of the paper also identified some limitations to these experiment settings. When the trained models are tested on a datasets of images that are different from those that were used for training, the overall accuracy was reduced to only 31\%. Other limitations of the dataset were also identified. The images were homogeneous, with plant leaves facing upward on a neutral background. Such a dataset does not reproduce real-world conditions and limits the applicability of models trained on it.

\textcite{Ferentinos:2018} trained five different CNN architectures - AlexNet, AlexNetOWTBn, GoogLeNet, Overfeat, and VGG - on an extended PlantVillage dataset. This extended dataset consisted of 87,306 leaf images of healthy and diseased plants. The 58 classes comprised in this dataset were as in (\cite{Mohanty:2016}) defined as a plant-disease pair or were dedicated solely to healthy leaves of a specific plant species. More than one third of the images in the dataset were captured under filed conditions. In comparison to the leaf images taken under controlled laboratory conditions, these were more complex in terms of the presence of irrelevant objects, other plant parts, multiple leaves, different background textures, and lighting conditions. The author then divided the initial database into a subset of training images and testing images in a ratio of 80\% to 20\%. The ratio of the images taken in controlled environment and the images taken under real-world conditions was kept the same in both subsets as it was in the original dataset. Another approach to developing the training and testing subsets was the preprocessing of the original dataset by rescaling and cropping the images. The ratio of 80\% to 20\% in terms of training and testing subsets was kept the same. All five architectures were trained using datasets created using both of the approaches. All the five models achieved success rates of over 97\% in both of the experiment settings. The highest success rate of 99.53\% was achieved by the VGG model, and the lowest overall average testing error of 0.0192 was achieved by the AlexNetOWTBn model. Both metrics were achieved on the original dataset. The author then used these two best performing models in another experiment setting to assess the models' ability to generalize between laboratory and field conditions. For this experiment setting, they focused solely on classes that contained images taken under both laboratory conditions and field conditions. From the 58 classes in the original dataset, only 12 contained images of both types. Using these 12 classes, the two models were trained only on the images taken under laboratory conditions and tested on images taken under real-world conditions. Then, the reversed training and testing strategy was applied. The VGG and AlexNetOWTBn models demonstrated varying performance under these experiment setups. When trained on field images and tested on laboratory images, the models achieved success rates of 65.69\% and 62.57\% respectively. When trained on laboratory images and tested on field images, the VGG and AlexNetOWTBn models achieved success rates of 33.27\% and 32.23\% respectively. The study results indicated that the CNN architectures were well suited for the task of classification of plant diseases or the absence thereof from plant leaf images. But as the authors in (\cite{Mohanty:2016}), this study also showed that classifying field images is more challenging than classifying laboratory images. This difficulty arises from increased complexity of field images that have varied backgrounds, inconsistent lightning conditions, and the presence of other for the classification irrelevant objects.

The authors in (\cite{Mohanty:2016}) and (\cite{Ferentinos:2018}) proposed that one of the practical applications of the trained deep learning models could be a mobile application for detecting and diagnosing plant diseases. Such an application would be particularly valuable for farmers in regions lacking the necessary infrastructure for early detection and treatment of plant diseases (\cite{Ferentinos:2018}). This tool could enable the farmers to identify and take appropriate measures at early developmental stages of plant diseases to ensure food safety (\cite{Mohanty:2016}). In both (\cite{Mohanty:2016}) and (\cite{Ferentinos:2018}), the authors concluded that in order to develop and train accurate plant disease classifiers a large amount of data is required. The results of both scientific papers showed that the data used for training must have grater variety and complexity in terms of background, lighting conditions, geographical locations, and cultivation conditions. \textcite{Ferentinos:2018} also stressed the technical feasibility of such an application on modern devices because the classification task requires relatively low computational power.

One of the first approaches to integrate a deep neural network into a mobile application for detection of plant diseases in environmental conditions was done by \cite{Picon:2019}. The work in (\cite{Picon:2019})is an extension of the previous research done in (\cite{Johannes:2017}). \textcite{Johannes:2017} created a mobile application that uses classical computer vision approach and machile learning techniques to classify early stages of diseases - septoria, rust, tan spot - simultaneosly in wheat leavs. The identification algorithm pipeline included an image preprocessing stage and a disease identification algorithm based on a primary segmentation module, Hot-Spot identifiaciton module, and a meta-classifier module. The primary segmentation module used a Naive Bayes classifier to identify potential disease-containing regions, hot-spots. The Hot-Spot identification module used two visual descriptors for color information and texture information. For every combination of disease and descriptor pair a Random-Forest based descriptor is trained assigning a feasibility value to the hot-spots. The meta-classifier is then used to compute a confidence score for the particular disease by considering different metrics yielded throughout the classification process. The authors used the AuC (the area under a receiver operating characteristic (ROC) curve) metric to evaluate the algorithm's performance. The AuC score ranges from 0 to 1. The higher the score is, the better. When changing the metadata-classifier threshold value ranging form 0 to 1, each point coordinates on the ROC-curve are represented the false positive rate at the x-axis and the true positive rate at the y-axis. The study achieved AuC scores above 0.80 for all three diseases in both k-fold validation0 and in pilot testing deployed in real mobile devices. The accuracy in both testing settings ranged from 73\% to 90\%. When comparing this approach with (\cite{Sladojevic:2016}), (\cite{Mohanty:2016}) or (\cite{Ferentinos:2018}), this classification algorithm performs well under real-world conditions. 

\textcite{Picon:2019} created a mobile application that uses a CNN-based approach to identify early disease stages and simultaneous diseases on wheat leafs. 

\end{document}